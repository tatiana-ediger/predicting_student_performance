{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads data\n",
    "features = pd.read_csv(\"data/features.csv\")\n",
    "target = pd.read_csv(\"data/target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins target values\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=4, encode = \"onehot-dense\", strategy = \"quantile\")\n",
    "\n",
    "target_discretized = discretizer.fit_transform(target[\"G3\"].values.reshape(-1, 1))\n",
    "target_new = []\n",
    "for row in target_discretized:\n",
    "    target_new.append(list(row).index(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data into training, testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets final grade for target\n",
    "t_train = y_train[\"G3\"]\n",
    "t_test = y_test[\"G3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "R-squared value for training set:  0.34042660318538653\nR-squared value for testing set:  0.22771777942292692\n"
    }
   ],
   "source": [
    "# Imports Ridge and accuracy metrics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Performs Ridge Regression\n",
    "model = Ridge().fit(X=X_train, y=t_train)\n",
    "\n",
    "# Measures accuracy\n",
    "print(\"R-squared value for training set: \", r2_score(t_train, model.predict(X_train)))\n",
    "print(\"R-squared value for testing set: \", r2_score(t_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data for classification into training, testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target_new, random_state = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction accuracy on the train data: 37.42%\nPrediction accuracy on the test data: 33.72%\n"
    }
   ],
   "source": [
    "# Imports SVC and accuracy metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Recommended using kernel\n",
    "\n",
    "# Performs SVC\n",
    "model = SVC(gamma = \"scale\").fit(X=X_train, y=y_train)\n",
    "\n",
    "# Measures accuracy\n",
    "accuracy_train = model.score(X_train, y_train)\n",
    "accuracy_test = model.score(X_test, y_test)\n",
    "print(\"Prediction accuracy on the train data:\", f\"{accuracy_train:.2%}\")\n",
    "print(\"Prediction accuracy on the test data:\", f\"{accuracy_test:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction accuracy on the train data: 100.00%\nPrediction accuracy on the test data: 38.70%\n"
    }
   ],
   "source": [
    "# Imports Decision Tree and accuracy metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Performs Decision Tree Classifier\n",
    "model = DecisionTreeClassifier().fit(X=X_train, y=y_train)\n",
    "\n",
    "# Recommended using kernel -> we want to be able to model nonlinear things!\n",
    "# Want to use feature expansion to classify them\n",
    "# To choose the kernel: brute force/ try out a bunch of different ones (look at data and see if there's anything interesting you can pick up on, for example see if there's any 2 or more things that you can take the product of and it seems interesting)\n",
    "# kernel: considering n features at the same time\n",
    "\n",
    "# Measures accuracy\n",
    "accuracy_train = model.score(X_train, y_train)\n",
    "accuracy_test = model.score(X_test, y_test)\n",
    "print(\"Prediction accuracy on the train data:\", f\"{accuracy_train:.2%}\")\n",
    "print(\"Prediction accuracy on the test data:\", f\"{accuracy_test:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, inputDim, outputDim, layerDim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.inputDim = inputDim\n",
    "        self.outputDim = outputDim\n",
    "        self.layerDim = layerDim\n",
    "\n",
    "\n",
    "        self.l1 = torch.nn.Linear(self.inputDim, self.layerDim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.l2 = torch.nn.Linear(self.layerDim, self.outputDim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.l1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.l2(relu)\n",
    "        # output = self.relu(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 96)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[3:6].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(96, 20, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1])"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(t_train.values[0]).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 20])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tensor = torch.FloatTensor(X_test.iloc[3:5].values)\n",
    "y_pred = model(x_test_tensor)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "99 1.6315577030181885\n199 1.5646674633026123\n299 1.5326740741729736\n399 1.5194493532180786\n499 1.5106877088546753\n"
    }
   ],
   "source": [
    "# Now have to loop through all my data points to optimize my model - for loop looping through groups of (5) data points at a time using indexing\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(y_pred, torch.tensor(t_train.values[0:2]).reshape(-1))\n",
    "\n",
    "# model.eval()\n",
    "# x_test_tensor = torch.FloatTensor(X_test.iloc[3].values[1:])\n",
    "# y_pred = model(x_test_tensor)\n",
    "# before_train = criterion(y_pred.squeeze(), torch.FloatTensor(t_test.values[0]))\n",
    "# print('Test loss before training' , before_train.item())\n",
    "\n",
    "# training mode vs. evaluation mode\n",
    "# different architecture @ training/eval times\n",
    "# model.train()\n",
    "# y_pred = model(torch.FloatTensor(X_train.iloc[3].values[1:]))\n",
    "# y_pred\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 2 inputs, first is model.parameters() - includes all parameters in the model you want to optimize, make sure model.parameters() includes everything you want to optimize\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    x_test_tensor = torch.FloatTensor(X_test.iloc[3:5].values)\n",
    "    y_pred = model(x_test_tensor)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = criterion(y_pred, torch.tensor(t_train.values[0:2]).reshape(-1))\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([16, 15,  5,  8, 15, 11,  7, 10, 14, 16,  9,  8,  5, 13,  8, 10, 11,\n       13, 12, 16, 14,  7, 15, 11,  8, 11, 11, 10, 12, 13,  9, 10, 12, 10,\n       12, 17,  9, 15, 16, 11, 16, 12, 16, 16, 16, 10, 13, 13, 13, 11, 11,\n       18, 16, 17,  8, 13,  8, 18, 16, 18, 16, 13, 12, 16, 10, 11, 10, 10,\n       12,  6, 16, 18,  8, 16, 13,  9, 12,  8, 11, 12, 11,  6,  9, 14,  0,\n       11,  9, 11, 10, 16, 10,  6, 12, 13, 10, 14, 10,  8,  6,  8, 15, 13,\n       10, 10, 12, 10, 11, 11, 11, 10, 10,  9, 15, 11, 19, 14,  0, 12, 13,\n       12, 10, 10, 12,  9,  9, 14, 11, 10, 10, 14, 11, 14, 11,  0, 14, 13,\n       14,  8, 12,  0,  7, 10, 10,  0, 12, 15, 12, 10, 14, 17, 13, 12,  6,\n       10, 15, 15,  8,  9, 11, 13, 11, 13, 13, 13, 10, 10, 11, 10, 12, 11,\n        8,  8,  0, 11,  7, 13, 12,  0, 11, 16, 13,  9,  8,  0, 18, 14, 14,\n       14, 11, 19, 11, 12, 10, 14, 11, 11, 11,  8,  9,  5, 11, 15, 10, 12,\n       10, 11, 16, 10, 10, 13, 14, 11, 14, 14, 15, 17, 10, 18, 17, 12, 12,\n        8, 10, 11, 11, 13, 10, 11, 15,  6, 10,  8,  0,  0, 11, 12, 15,  0,\n       12, 13, 10, 12,  6,  0, 16, 14, 13, 14,  0, 11,  9, 11, 11, 11, 11,\n        0, 15, 12, 14, 15, 16,  8, 12, 13, 17,  8, 14, 14, 14, 14,  8, 10,\n       10,  0, 13,  8, 16,  9, 10,  8,  0, 14,  7, 15, 16, 10, 12, 15, 10,\n       17, 10,  8, 14, 16,  0,  8, 15,  8, 11,  0, 14, 17,  0, 14, 15, 12,\n       18, 11, 10, 15,  8,  8, 17, 10, 10, 14,  0,  8, 10, 12, 12, 14, 12,\n       10,  0, 12, 13, 11, 17, 11, 13,  9, 18,  5, 15,  9, 18,  9,  9,  6,\n       18, 10, 19,  0, 13, 16, 18, 11, 10,  8, 16, 10, 12, 10, 13, 15,  9,\n       10,  0, 12, 10, 17, 11, 10, 11, 11, 11,  8,  1, 18,  9, 16, 17, 13,\n       15, 10, 12,  9, 11, 11, 13, 16, 16,  9,  0, 14, 11, 11, 12, 15, 10,\n       15, 11, 13, 17, 12, 10,  0, 18, 14, 18,  8, 13, 14, 13, 12, 14, 15,\n       12, 10, 11, 11, 16,  9, 11, 13, 13, 17, 16, 14, 12, 14, 14, 17, 11,\n       15, 18, 12, 10, 11, 12, 13, 12, 16,  0, 10, 10, 10, 18, 11, 14, 15,\n       10, 13, 15, 12, 11,  7, 15,  8,  6,  6, 10,  6, 10, 12, 12, 18, 11,\n       14, 11, 13, 11, 10, 12, 17, 10, 10, 10, 10,  8, 12, 11, 10,  7, 11,\n       11, 15, 12, 13, 13,  9, 10, 13, 10, 10, 13, 11, 10, 14,  6, 13, 11,\n       15, 17, 19, 12, 14, 11, 10, 12, 12, 10, 16,  8, 11, 14, 10,  9, 13,\n       18,  0,  7, 15, 11, 13, 13, 13, 11, 14, 12, 10, 13, 14, 14,  9, 12,\n        9,  0, 17, 13, 15, 17,  8, 14, 12,  9,  9, 15, 11, 15, 10, 10, 15,\n       18, 16, 11, 17,  7, 15,  0,  8,  9, 10, 15, 15,  6, 15, 11, 15, 15,\n        7, 13, 13, 12, 10, 16, 13, 16, 10, 16, 10,  7, 14,  8,  8, 17, 12,\n        9, 12,  7,  0, 10, 15, 11,  6, 12, 10, 10, 14, 10, 14, 11, 12, 11,\n       15,  0, 11, 13, 12, 10, 10,  0,  9, 11, 12,  0, 11,  5, 15, 15, 13,\n       11,  0,  7, 12,  9, 10, 10, 12, 12, 15,  9,  5, 13, 12,  8,  8, 11,\n       13, 10,  0, 11,  8, 14, 15, 11, 12,  5, 10, 12, 17,  9,  7, 15, 11,\n       15, 18, 10, 11, 13, 11, 13, 10, 11, 15,  8, 15,  0, 12, 16, 13,  6,\n       14, 15, 14, 11, 11, 14, 11, 13, 14, 16, 11,  7,  9,  9, 13, 19,  9,\n       11, 15, 15, 13, 12, 16, 13, 10, 11, 12, 11, 17, 10, 11, 15, 17, 12,\n       13, 13, 13, 14,  9, 13, 15, 10,  9, 12, 10,  9,  8, 14, 14, 14, 13,\n        6, 11, 11, 11,  0, 13,  9, 10,  8, 14, 13, 11, 11,  9, 13, 12, 13,\n       10, 12, 11, 10, 15, 11, 11, 11, 15, 13,  9, 11, 11, 10, 17, 14, 17,\n       14, 12, 11,  8, 15, 11, 13, 11, 16, 10, 10, 17, 15, 13, 12, 10, 10,\n       10, 11,  9, 13, 12, 10, 12, 11, 13, 13,  7, 15, 16, 12, 12, 19, 17,\n        9])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}