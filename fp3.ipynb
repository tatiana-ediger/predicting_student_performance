{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads data\n",
    "features = pd.read_csv(\"data/features.csv\")\n",
    "target = pd.read_csv(\"data/target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins target values\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=4, encode = \"onehot-dense\", strategy = \"quantile\")\n",
    "\n",
    "target_discretized = discretizer.fit_transform(target[\"G3\"].values.reshape(-1, 1))\n",
    "target_new = []\n",
    "for row in target_discretized:\n",
    "    target_new.append(list(row).index(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data into training, testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets final grade for target\n",
    "t_train = y_train[\"G3\"]\n",
    "t_test = y_test[\"G3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "R-squared value for training set:  0.34042660318538653\nR-squared value for testing set:  0.22771777942292692\n"
    }
   ],
   "source": [
    "# Imports Ridge and accuracy metrics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Performs Ridge Regression\n",
    "model = Ridge().fit(X=X_train, y=t_train)\n",
    "\n",
    "# Measures accuracy\n",
    "print(\"R-squared value for training set: \", r2_score(t_train, model.predict(X_train)))\n",
    "print(\"R-squared value for testing set: \", r2_score(t_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data for classification into training, testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target_new, random_state = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction accuracy on the train data: 37.42%\nPrediction accuracy on the test data: 33.72%\n"
    }
   ],
   "source": [
    "# Imports SVC and accuracy metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Recommended using kernel\n",
    "\n",
    "# Performs SVC\n",
    "model = SVC(gamma = \"scale\").fit(X=X_train, y=y_train)\n",
    "\n",
    "# Measures accuracy\n",
    "accuracy_train = model.score(X_train, y_train)\n",
    "accuracy_test = model.score(X_test, y_test)\n",
    "print(\"Prediction accuracy on the train data:\", f\"{accuracy_train:.2%}\")\n",
    "print(\"Prediction accuracy on the test data:\", f\"{accuracy_test:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction accuracy on the train data: 100.00%\nPrediction accuracy on the test data: 36.78%\n"
    }
   ],
   "source": [
    "# Imports Decision Tree and accuracy metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Performs Decision Tree Classifier\n",
    "model = DecisionTreeClassifier().fit(X=X_train, y=y_train)\n",
    "\n",
    "# Recommended using kernel -> we want to be able to model nonlinear things!\n",
    "# Want to use feature expansion to classify them\n",
    "# To choose the kernel: brute force/ try out a bunch of different ones (look at data and see if there's anything interesting you can pick up on, for example see if there's any 2 or more things that you can take the product of and it seems interesting)\n",
    "# kernel: considering n features at the same time\n",
    "\n",
    "# Measures accuracy\n",
    "accuracy_train = model.score(X_train, y_train)\n",
    "accuracy_test = model.score(X_test, y_test)\n",
    "print(\"Prediction accuracy on the train data:\", f\"{accuracy_train:.2%}\")\n",
    "print(\"Prediction accuracy on the test data:\", f\"{accuracy_test:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, inputDim, outputDim, layerDim):\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.inputDim = inputDim\n",
    "        self.outputDim = outputDim\n",
    "        self.layerDim = layerDim\n",
    "\n",
    "\n",
    "        self.l1 = torch.nn.Linear(self.inputDim, self.layerDim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.l2 = torch.nn.Linear(self.layerDim, self.outputDim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.l1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.l2(relu)\n",
    "        # output = self.relu(output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 96)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[3:6].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 0.0004,  0.0139,  0.1010,  ...,  0.0185, -0.0828, -0.0452],\n        [ 0.0258,  0.0610,  0.0891,  ..., -0.0968, -0.0176,  0.0666],\n        [-0.0256,  0.0065,  0.0219,  ..., -0.0348,  0.0105,  0.0557],\n        ...,\n        [-0.0786, -0.0832,  0.0057,  ..., -0.0413,  0.0473, -0.0938],\n        [-0.0646, -0.0600, -0.0633,  ..., -0.0691, -0.0881, -0.0751],\n        [ 0.0054,  0.0450,  0.0137,  ..., -0.0528, -0.0843, -0.0382]])\ntensor([ 0.0996, -0.0487,  0.0365,  0.0181,  0.0419, -0.0531, -0.0529, -0.0499,\n         0.0630,  0.0954,  0.0784,  0.0680,  0.0020,  0.0160, -0.0736,  0.0307,\n        -0.0902, -0.0586,  0.0978, -0.0893,  0.0395,  0.0879, -0.0726, -0.0165,\n        -0.0488, -0.0735,  0.0473, -0.0660,  0.0933,  0.0236, -0.0434, -0.0891,\n         0.0850, -0.0960,  0.0679,  0.0499,  0.0247,  0.0092,  0.0162, -0.0167,\n         0.0099,  0.0418,  0.0605, -0.0702, -0.0654, -0.0005, -0.0111,  0.0303])\ntensor([[ 1.7633e-03, -5.5875e-02, -3.6361e-03,  5.1476e-02,  4.6694e-02,\n          1.3061e-01, -5.2011e-02,  9.9111e-02, -6.2960e-02,  7.2130e-02,\n          6.3603e-02, -8.0358e-02,  4.7582e-02,  1.2088e-01, -6.7634e-02,\n          6.2738e-02, -1.2302e-02,  2.8838e-03,  2.8947e-02, -1.2563e-01,\n         -1.3434e-02, -2.3884e-02, -9.8092e-02, -6.3446e-02,  1.0815e-02,\n          7.3447e-03, -1.4081e-01,  2.5660e-02,  4.1319e-02,  9.9291e-02,\n          1.3774e-01, -1.1992e-01, -3.7787e-02,  1.0072e-01, -1.3400e-01,\n          6.4852e-02,  1.2276e-01,  2.5687e-02, -1.2870e-01,  2.4116e-02,\n         -1.0695e-01,  7.0192e-02, -2.2405e-02, -9.7410e-02,  1.2396e-01,\n         -1.3967e-02, -2.0268e-02,  4.5165e-02],\n        [ 8.4853e-02, -1.3242e-01, -1.3518e-01, -9.7454e-02,  6.2799e-02,\n         -1.0393e-01, -3.7163e-02, -1.0808e-01,  7.1072e-02,  4.4069e-02,\n         -7.2667e-03, -1.4427e-01, -3.6482e-02, -1.3576e-01,  1.2498e-01,\n         -1.0538e-02, -9.0497e-02,  7.0781e-02, -8.5118e-02, -6.4947e-02,\n         -8.5559e-02, -9.6172e-02, -1.3729e-01,  1.4096e-01, -1.4312e-01,\n         -9.6163e-03,  3.6957e-02, -4.7073e-02,  3.1064e-02, -3.3681e-02,\n          7.5847e-02, -9.0353e-03, -3.1182e-03,  5.7901e-02,  6.8958e-02,\n          4.4749e-02, -8.1787e-02,  1.4389e-01,  1.3173e-01,  6.3278e-03,\n         -6.5481e-02,  1.2912e-01,  1.5419e-03, -8.3982e-02, -7.2341e-03,\n          1.1496e-01, -1.2025e-02,  1.1340e-01],\n        [-7.5084e-02,  7.1377e-02, -1.2097e-01,  1.2997e-01,  7.8451e-02,\n          8.1324e-02, -8.1065e-02,  2.4699e-02, -4.0148e-02, -1.2393e-01,\n         -1.0633e-01,  1.2104e-01, -7.6667e-02,  1.2627e-01,  1.6069e-02,\n          8.1309e-02, -2.0609e-03, -6.7568e-02,  3.6117e-02,  9.7791e-02,\n          1.0351e-01, -1.1697e-01, -9.3642e-02,  1.1145e-02,  2.2791e-03,\n         -1.4354e-01, -1.3941e-01, -1.2565e-01, -7.5658e-02,  1.1981e-01,\n          1.2391e-01, -9.1629e-02, -1.3724e-01, -6.6072e-02, -1.0196e-01,\n          1.0752e-01, -8.8031e-02, -1.3599e-01, -8.6618e-02,  1.1866e-01,\n         -3.1758e-02, -6.4753e-02,  2.7078e-02,  4.6132e-02,  2.4129e-02,\n          5.4418e-02, -1.7981e-02, -4.0819e-02],\n        [-6.1757e-02, -1.2866e-01,  4.1140e-02, -8.4870e-02, -1.2207e-01,\n          1.2230e-01,  1.3089e-02, -1.3673e-02,  1.2450e-01,  1.2071e-01,\n         -5.6065e-02,  1.0729e-01, -4.0154e-02,  9.9045e-02,  7.9442e-02,\n         -1.0753e-01,  7.8043e-02, -5.7358e-02, -7.4022e-03,  3.4392e-02,\n          6.8518e-02, -8.9519e-02, -1.2801e-01, -1.0540e-01,  1.3459e-01,\n          3.8330e-02, -1.1141e-01,  1.1478e-01,  7.2372e-02,  1.2204e-02,\n         -1.5657e-02,  8.2858e-02, -2.1207e-02, -9.2408e-02,  2.6266e-02,\n          4.7554e-02, -6.5507e-02, -1.2488e-01, -9.5434e-02,  8.0070e-02,\n         -1.2785e-01, -3.2829e-03,  1.2015e-01, -1.2594e-02,  3.4890e-02,\n          1.1630e-01,  6.5516e-02, -1.6560e-02],\n        [ 1.5592e-02, -1.2106e-01,  4.4587e-02, -3.0791e-02, -1.2129e-01,\n         -1.4302e-01, -8.0485e-02, -9.4624e-02, -9.2184e-02, -1.3910e-01,\n         -1.0338e-01, -7.3048e-02, -7.5602e-03, -1.2106e-01,  1.1534e-01,\n          8.3647e-02,  7.6376e-02, -1.1551e-01,  6.6140e-03, -9.4661e-02,\n          1.2302e-01,  3.4493e-02, -3.8201e-02,  4.0969e-02,  4.8974e-02,\n          9.7654e-02, -2.6207e-02,  1.4089e-01,  9.1468e-02, -7.9846e-02,\n          1.3423e-01,  1.4018e-01,  5.1763e-02, -3.4824e-02,  1.1856e-01,\n          8.2246e-02,  1.4248e-02, -3.8955e-02,  8.2326e-02, -3.5425e-03,\n          1.2672e-01,  1.3630e-02, -3.2076e-02, -8.0915e-02,  2.4297e-02,\n         -6.9438e-02, -1.7916e-02, -2.1591e-02],\n        [ 1.0335e-01,  1.1335e-01,  4.2016e-03,  1.1435e-01,  1.2234e-01,\n          3.3270e-02,  2.9176e-02,  1.1822e-01,  6.0944e-02,  1.2084e-01,\n         -1.4431e-01, -6.8902e-02, -9.7638e-02, -3.9390e-02,  4.4693e-02,\n         -8.7576e-02,  1.1971e-01, -1.2466e-01,  1.1493e-01, -9.7520e-02,\n          1.1568e-01,  1.5671e-02,  6.1019e-02, -4.7137e-02,  6.0003e-02,\n          1.1197e-01,  9.9889e-02, -1.1255e-01,  1.0346e-01,  1.2041e-01,\n         -9.8745e-03,  1.1417e-02,  9.2721e-02, -6.0768e-02,  8.3042e-02,\n          2.2584e-02, -9.0891e-02,  1.3378e-01, -1.2202e-01, -3.6673e-02,\n         -1.3107e-02,  6.6332e-03, -1.0884e-01,  2.6567e-02, -8.2331e-02,\n         -2.7407e-03, -4.9609e-02, -3.6916e-02],\n        [ 5.6211e-02,  1.1159e-01,  1.0132e-01,  7.8736e-02,  8.3663e-02,\n          1.7966e-02,  7.9168e-02, -1.7122e-02,  6.6765e-02, -1.9279e-02,\n         -1.7927e-02, -3.0492e-02,  6.9268e-02, -8.8752e-02, -1.3458e-01,\n         -8.5480e-02,  4.6200e-02, -1.3297e-01,  1.3235e-01, -8.9730e-02,\n         -7.2232e-02, -4.7894e-02,  7.1846e-04,  1.0133e-01, -1.4009e-01,\n         -1.8386e-02, -5.6106e-04, -1.7363e-02, -5.6121e-02,  4.3274e-02,\n          3.9840e-02,  7.6164e-02, -1.2443e-01,  1.1411e-01, -1.3874e-01,\n         -1.2815e-04,  8.4408e-02,  1.2859e-01, -4.4165e-03, -8.0931e-02,\n          1.0587e-01, -4.6501e-02, -2.3934e-02,  1.3471e-01, -9.7134e-02,\n          5.1229e-03,  1.2903e-01,  3.4271e-02],\n        [ 1.4748e-02, -1.2126e-02, -4.2860e-02,  5.4156e-02,  1.3954e-02,\n          1.2349e-01,  1.0321e-01, -6.8180e-02, -6.1641e-02, -5.0472e-02,\n         -6.7505e-02, -6.8596e-02, -5.4973e-02,  8.6316e-02,  1.0109e-01,\n         -6.9973e-02, -8.6268e-02,  5.0025e-02, -8.0901e-02, -1.2003e-01,\n          1.0732e-01, -3.4535e-02,  1.3564e-01, -5.4064e-02, -1.0249e-01,\n         -1.0653e-01, -2.4050e-02, -7.1240e-02, -1.3230e-01, -5.2275e-02,\n         -5.1273e-02,  1.9910e-02,  8.7705e-02,  1.2926e-01,  3.5278e-02,\n         -9.5876e-02, -1.1113e-01,  9.7753e-02,  2.2731e-02,  8.8242e-02,\n          6.1031e-02,  8.2232e-02,  7.8599e-02,  3.5779e-02, -6.1308e-02,\n          4.0831e-02,  1.2517e-01,  9.5657e-02],\n        [-4.5661e-02, -5.0569e-02, -8.9917e-02,  1.4081e-01, -1.2789e-01,\n          9.8273e-02, -6.1185e-02,  1.1439e-01, -1.0998e-02, -9.1784e-02,\n         -1.9548e-02,  1.0013e-01,  8.0231e-02,  7.5986e-02,  5.6044e-02,\n          4.4005e-02,  8.9324e-02, -1.1424e-01, -7.3907e-02,  5.0951e-02,\n          3.4458e-02, -4.8172e-02,  2.4259e-03, -6.5605e-02, -1.4836e-02,\n          6.2614e-02, -9.1527e-02,  4.1348e-02, -1.0996e-01,  5.2965e-02,\n          4.8953e-02, -1.0020e-02, -2.3818e-02,  9.8065e-02,  1.1975e-01,\n         -5.1247e-02,  1.4082e-01, -4.8856e-02,  6.1869e-02, -7.1637e-03,\n          5.2124e-02,  9.7234e-02, -1.0828e-01, -3.9584e-02,  1.3246e-01,\n         -2.3834e-02, -1.2492e-01, -9.4834e-02],\n        [-1.1509e-01,  9.8196e-02,  1.3939e-01, -1.2117e-01,  1.0970e-01,\n          9.6654e-02, -1.1952e-02, -4.4123e-02,  1.2758e-01,  7.3865e-02,\n         -4.3154e-02,  9.1661e-02,  9.4022e-02,  7.0510e-02, -1.0501e-01,\n          3.4662e-02,  7.7318e-02, -6.1049e-02,  8.0018e-02, -5.2695e-02,\n         -5.7617e-03, -9.7003e-02, -2.5355e-03, -1.2436e-01, -1.3865e-01,\n          1.4104e-01, -1.1991e-01, -8.6371e-02, -1.4674e-02, -5.2872e-02,\n          9.9763e-02,  8.6139e-02,  2.1697e-02, -1.2772e-01,  9.7522e-02,\n         -1.3429e-01, -1.3674e-01, -2.7302e-02, -5.7095e-02,  1.2242e-01,\n         -1.6467e-02,  8.2124e-02, -1.1039e-01, -1.4175e-01,  1.1725e-01,\n          1.3837e-01,  8.6349e-02, -6.7263e-02],\n        [ 1.1032e-01, -1.0281e-01,  2.9338e-02, -6.1169e-02,  3.9830e-02,\n          1.2477e-01,  9.6188e-02,  8.5034e-02,  3.8790e-02,  9.8044e-02,\n         -6.9496e-02,  9.8245e-02,  1.4220e-01, -8.2571e-02, -2.8170e-03,\n          1.1591e-01, -1.1084e-01,  5.1311e-02, -9.3246e-02,  7.5959e-02,\n         -1.3956e-01, -9.8534e-02, -2.1437e-02,  7.2076e-02,  4.4077e-02,\n         -3.6982e-02,  1.0599e-02,  3.5840e-04,  5.8326e-02, -7.5508e-02,\n         -3.0965e-02,  4.3308e-02, -1.1096e-01,  9.8925e-02,  1.2737e-02,\n          1.1618e-01,  2.0465e-02,  1.3202e-01,  9.1897e-03, -5.0868e-03,\n          1.1384e-01, -4.6852e-02,  6.1839e-02, -5.7538e-02, -1.2653e-01,\n          9.4647e-02, -1.1092e-01, -1.0103e-02],\n        [-1.9162e-02, -1.2562e-01, -2.8677e-02,  1.4085e-01,  6.2248e-02,\n         -9.4070e-02,  8.1267e-02, -5.2511e-03,  7.3899e-02,  1.1690e-01,\n          9.8131e-02,  9.5542e-02,  6.4967e-02, -9.0196e-02,  8.0101e-03,\n         -8.1687e-02,  1.0321e-01,  1.5350e-02, -1.0246e-01,  1.9272e-02,\n          8.9944e-02,  9.5693e-02,  1.2989e-01, -8.3985e-02,  8.1783e-02,\n          2.1012e-02,  5.7218e-02,  9.3194e-02,  3.3565e-02,  8.7521e-02,\n         -3.7362e-02, -9.6311e-02,  9.9285e-02, -5.6002e-02,  8.8753e-02,\n          1.3317e-01,  9.5835e-03,  1.3283e-01,  5.8635e-02, -1.1340e-01,\n          2.5782e-02,  1.5546e-02,  4.6193e-02,  2.1077e-02,  2.4453e-02,\n          3.3758e-02,  7.0425e-02, -8.5461e-02],\n        [-1.2333e-01,  6.2780e-03, -1.2938e-01, -1.1935e-01, -2.2806e-02,\n         -1.0074e-01,  1.1449e-01,  1.3043e-01,  9.8624e-02,  3.6314e-02,\n         -4.8287e-02, -1.2375e-01, -3.1913e-02,  8.0042e-02, -5.4689e-02,\n         -7.7418e-02, -3.6645e-02, -1.3079e-02, -5.8305e-02,  1.2754e-01,\n          9.2043e-02,  1.2879e-02,  3.3236e-03, -1.0654e-04,  6.5295e-02,\n         -8.2275e-02, -7.2539e-02, -4.4089e-02, -3.4340e-02, -4.6425e-03,\n          1.4057e-01,  7.7477e-02, -7.0952e-02, -1.7416e-02,  5.6489e-03,\n          5.8476e-02, -1.2014e-02,  1.3029e-01,  8.7402e-02, -2.1020e-02,\n          1.0327e-01, -7.4216e-02,  8.5337e-02,  9.2899e-02, -1.2146e-01,\n         -1.1701e-02,  6.2574e-02, -1.2795e-01],\n        [-1.0657e-01, -7.1932e-03, -5.4545e-02,  4.2233e-02,  1.1642e-01,\n          1.2843e-02, -1.0355e-01,  4.9087e-02, -1.3450e-02, -7.8336e-02,\n          7.1450e-02, -1.1176e-01,  8.1231e-02,  4.4606e-02, -4.2535e-02,\n          8.2827e-02,  8.5501e-02,  2.0671e-02, -1.1081e-01,  1.4421e-01,\n          4.1771e-02, -7.0194e-02,  2.9592e-02,  6.7844e-02, -1.0439e-01,\n         -4.8949e-02,  1.3783e-01,  2.5439e-02, -1.2149e-01,  5.8550e-02,\n          1.4348e-01,  1.0547e-02,  3.4435e-02,  1.3304e-01, -1.2751e-01,\n         -4.8776e-02,  7.3736e-02, -1.0521e-01, -4.6472e-02, -3.1587e-02,\n          9.5805e-02, -1.1280e-02, -7.7349e-02,  1.4052e-01,  5.7639e-03,\n          9.6548e-03, -7.9074e-02, -6.7654e-02],\n        [-2.6250e-02, -8.0603e-02,  3.7816e-02,  1.2879e-01,  4.4615e-02,\n          4.1182e-02,  5.6387e-03,  7.2253e-02, -4.1585e-02,  3.7140e-02,\n         -1.3084e-01, -1.0183e-01,  5.9114e-02, -1.2445e-01, -2.5316e-02,\n          1.3759e-01,  6.1110e-02,  2.4005e-02, -2.7973e-02,  6.8411e-02,\n          2.7733e-03, -1.3604e-01,  8.9164e-02, -1.2010e-01,  5.4668e-03,\n         -4.4509e-02, -1.2957e-01,  4.9589e-02,  9.4392e-02,  1.1928e-01,\n          4.3764e-02,  6.0462e-02,  1.2293e-01,  4.4989e-02, -5.5832e-02,\n         -1.2865e-01,  7.0276e-03, -4.6181e-02,  1.7478e-02, -1.2734e-01,\n          6.0998e-02,  6.1601e-02, -9.7945e-02, -2.7156e-02, -7.6002e-02,\n          8.5715e-02,  5.7106e-02,  1.4074e-01],\n        [-1.2194e-01,  5.5639e-02, -8.8007e-02,  3.6888e-02, -5.2250e-02,\n          1.1306e-02, -1.3054e-01, -7.9660e-02,  1.3970e-01,  1.4012e-01,\n         -8.1700e-02, -1.0972e-01, -4.9474e-02,  8.9543e-02, -1.3785e-01,\n          1.2123e-01, -3.0847e-02, -3.1575e-02,  5.5278e-02, -1.4181e-01,\n         -1.3654e-01, -6.6266e-02, -9.4079e-02, -6.4428e-02,  7.7790e-02,\n         -4.8844e-02, -2.5531e-02,  2.4659e-02, -1.4143e-01, -4.5755e-02,\n          6.6317e-02, -8.9360e-02, -1.3757e-01, -1.3441e-01, -2.8693e-02,\n          1.2363e-02,  6.7720e-02, -3.9552e-02, -1.4343e-01,  1.1446e-01,\n          7.4062e-02,  6.5316e-02, -4.3100e-02, -6.2256e-03,  9.0242e-03,\n          1.4358e-01,  3.1225e-02, -8.0265e-02],\n        [ 1.0596e-01, -3.2817e-02,  1.3583e-01, -8.7451e-02,  5.4129e-02,\n         -3.5022e-02,  1.0087e-02,  9.1901e-03, -3.3697e-02, -1.0556e-01,\n         -3.6291e-02,  1.2282e-02,  2.2225e-02,  2.0448e-02, -1.2665e-01,\n         -1.0980e-01, -1.3544e-01,  1.1013e-03, -1.3418e-01,  6.0766e-02,\n         -1.1338e-01, -8.8157e-02,  3.1038e-02,  1.3569e-01,  6.3576e-02,\n         -2.3502e-02,  6.8214e-02,  4.0667e-02,  2.6024e-02,  5.4062e-03,\n          1.3633e-01,  3.5355e-02,  3.7496e-02,  1.0703e-01, -4.5915e-02,\n         -3.5146e-02, -6.4961e-02, -9.6616e-02, -1.4586e-03, -4.5315e-02,\n         -5.4417e-02,  1.2425e-01, -8.7387e-02, -3.4944e-02, -1.2380e-01,\n          1.9017e-02, -4.2517e-02, -5.7668e-02],\n        [-1.2209e-01, -3.3751e-02, -1.2480e-01, -6.5844e-02, -2.8606e-02,\n         -1.9114e-02,  1.3233e-01,  1.1821e-01,  1.2192e-01, -6.0174e-03,\n          1.2614e-03,  6.8228e-02, -1.3714e-01, -1.0314e-01, -5.6175e-02,\n         -2.9789e-02,  1.3741e-01,  7.9334e-02,  8.0179e-02, -9.1779e-02,\n          1.0424e-01, -4.2294e-02, -7.3945e-02, -1.2072e-01,  5.8745e-02,\n          4.9544e-02, -8.9407e-02, -4.1572e-03,  1.1882e-02, -1.2073e-01,\n         -1.3503e-01, -7.3427e-02,  1.2755e-01, -6.1413e-02, -5.9782e-02,\n         -3.4376e-02, -7.3294e-02,  1.2177e-01, -8.3333e-04, -8.7711e-02,\n         -6.0103e-02,  3.8188e-02,  3.1336e-02,  1.1495e-01,  9.5990e-02,\n          3.2146e-02, -1.0076e-01,  1.0110e-01],\n        [ 2.3681e-02,  4.1264e-02,  5.0257e-02,  4.0141e-02,  9.3918e-02,\n          5.7874e-02, -1.9376e-02,  1.3446e-01,  1.2327e-02, -1.0658e-01,\n         -1.4179e-01, -1.3040e-01,  8.8502e-02,  1.3985e-01,  2.2298e-02,\n         -4.3745e-02,  8.4598e-02,  3.7529e-03,  8.2654e-02, -1.0665e-01,\n          7.5820e-02,  1.1679e-01,  2.0364e-02,  1.5529e-02,  1.5029e-02,\n          1.2325e-02,  5.3357e-02, -1.0797e-01, -1.0181e-01, -4.2520e-02,\n         -4.6878e-02,  6.1917e-02, -5.1948e-02,  1.1142e-01,  5.7431e-02,\n         -5.2520e-02, -4.3648e-02,  6.7850e-02, -8.9564e-02,  3.6288e-02,\n          1.1434e-01, -4.0686e-02, -8.9169e-02,  3.2124e-02,  4.2639e-02,\n          9.5324e-02,  2.5598e-02,  3.4973e-02],\n        [ 1.1591e-02, -1.2635e-01,  9.6943e-02, -8.4258e-02, -7.0567e-02,\n          1.2096e-01,  1.3121e-01,  2.0867e-02, -8.4473e-02,  2.8238e-02,\n         -3.4389e-02, -7.1443e-02,  1.1181e-02, -1.4267e-01,  1.3140e-01,\n          1.0890e-01,  1.2906e-02,  1.3554e-01, -1.2518e-01, -4.6813e-02,\n          8.8009e-02,  1.2631e-01,  4.6385e-02, -6.5018e-02,  8.6782e-03,\n          1.4377e-01,  2.1328e-02, -7.7449e-02,  1.1843e-01,  3.8275e-02,\n         -1.0059e-01, -3.6163e-02,  6.6809e-02,  1.3660e-01, -1.3328e-01,\n         -1.2056e-01, -2.9181e-02,  9.9449e-02,  1.4247e-01, -1.3490e-01,\n          1.1398e-01,  1.1029e-01,  1.1891e-01,  1.1395e-01, -6.6391e-02,\n          1.1256e-01,  2.4123e-02, -1.0146e-01]])\ntensor([-0.0150,  0.0607,  0.0084, -0.0738,  0.1175, -0.0482,  0.0369,  0.0395,\n         0.0291,  0.0788, -0.1357, -0.1276,  0.1187, -0.1352, -0.1122,  0.1443,\n         0.1359, -0.0401,  0.0190,  0.0104])\n"
    }
   ],
   "source": [
    "model = NeuralNetwork(96, 20, 48)\n",
    "for param in model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(t_train.values[0]).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 20])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tensor = torch.FloatTensor(X_test.iloc[3:5].values)\n",
    "y_pred = model(x_test_tensor)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "99 6.123427867889404\n199 3.3155951499938965\n299 2.5472023487091064\n399 2.4920718669891357\n499 2.456698417663574\n"
    }
   ],
   "source": [
    "# Now have to loop through all my data points to optimize my model - for loop looping through groups of (5) data points at a time using indexing\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(y_pred, torch.tensor(t_train.values[0:2]).reshape(-1))\n",
    "\n",
    "# model.eval()\n",
    "# x_test_tensor = torch.FloatTensor(X_test.iloc[3].values[1:])\n",
    "# y_pred = model(x_test_tensor)\n",
    "# before_train = criterion(y_pred.squeeze(), torch.FloatTensor(t_test.values[0]))\n",
    "# print('Test loss before training' , before_train.item())\n",
    "\n",
    "# training mode vs. evaluation mode\n",
    "# different architecture @ training/eval times\n",
    "# model.train()\n",
    "# y_pred = model(torch.FloatTensor(X_train.iloc[3].values[1:]))\n",
    "# y_pred\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# 2 inputs, first is model.parameters() - includes all parameters in the model you want to optimize, make sure model.parameters() includes everything you want to optimize\n",
    "\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    x_train_tensor = torch.FloatTensor(X_train.values)\n",
    "    y_pred = model(x_train_tensor)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = criterion(y_pred, torch.tensor(t_train.values).reshape(-1))\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tensor = torch.FloatTensor(X_test.values)\n",
    "test_results = model.forward(x_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "06, -30.0414, -47.0334,  -6.1454,  -5.1790,\n         -4.6003,  -3.7558,  -3.8424,  -2.4967,  -2.8050,  -3.2492,  -3.2053,\n         -3.2211,  -3.3885,  -3.7393,  -3.9647,  -4.2647,  -5.7341],\n       grad_fn=<SelectBackward>)\n \ntensor([ -0.7600,  -2.6097, -12.1197,  -9.5193, -14.6477,  -1.9469,  -1.3182,\n         -2.3879,  -1.4975,  -1.0218,  -0.3144,  -0.4553,  -0.9143,  -1.1001,\n         -0.6654,  -0.8859,  -1.1131,  -1.5938,  -2.1520,  -3.0025],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.5967,  -4.1463, -18.6346, -14.5676, -22.6482,  -2.9931,  -2.3920,\n         -3.0865,  -2.0529,  -1.7341,  -0.6930,  -0.7871,  -1.4074,  -1.7213,\n         -1.5387,  -1.5824,  -1.8977,  -2.2164,  -2.7960,  -3.6900],\n       grad_fn=<SelectBackward>)\n \ntensor([ -3.6643,  -7.8569, -35.4357, -27.8251, -43.6609,  -5.6383,  -5.1226,\n         -4.2853,  -3.5308,  -3.5479,  -2.7160,  -2.4181,  -2.9719,  -3.0295,\n         -2.9479,  -3.3496,  -3.4100,  -3.3953,  -4.2742,  -5.3172],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.6087,  -4.0445, -16.7446, -13.4373, -20.5300,  -2.6581,  -2.0549,\n         -2.5852,  -1.8088,  -1.5244,  -1.0778,  -0.7156,  -1.2314,  -1.2652,\n         -1.0824,  -1.3514,  -1.6411,  -1.7176,  -2.2995,  -3.1516],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.6857, -13.8621, -62.1910, -48.9193, -76.8635,  -9.9520,  -8.9989,\n         -7.0298,  -5.9671,  -5.9593,  -4.8576,  -5.0173,  -5.0914,  -5.2145,\n         -5.8849,  -5.5183,  -6.0264,  -6.3267,  -6.6698,  -8.5419],\n       grad_fn=<SelectBackward>)\n \ntensor([ -4.2944,  -9.1670, -41.1561, -32.7105, -50.6447,  -6.6788,  -5.6553,\n         -4.9778,  -4.1648,  -4.2198,  -3.2783,  -2.9684,  -3.1928,  -3.1501,\n         -3.5796,  -3.5867,  -3.9250,  -4.1038,  -4.6408,  -5.9557],\n       grad_fn=<SelectBackward>)\n \ntensor([ -5.0476, -11.0258, -49.9725, -39.1707, -61.3245,  -8.0265,  -7.0804,\n         -5.9898,  -4.6474,  -4.8247,  -3.5781,  -3.6336,  -4.1064,  -4.0378,\n         -4.4779,  -4.5858,  -4.7568,  -5.1654,  -5.6535,  -7.2048],\n       grad_fn=<SelectBackward>)\n \ntensor([ -7.2562, -15.1309, -68.3071, -53.4682, -84.1728, -10.6233,  -9.9652,\n         -7.6088,  -6.2239,  -6.1984,  -5.1516,  -5.3035,  -5.8318,  -5.9182,\n         -6.2102,  -6.1268,  -6.8069,  -7.0294,  -7.2333,  -9.6048],\n       grad_fn=<SelectBackward>)\n \ntensor([ -0.7065,  -2.5686, -11.6732,  -9.0306, -14.2136,  -1.7801,  -1.4606,\n         -2.2898,  -1.3057,  -0.8215,  -0.0522,  -0.3979,  -0.9440,  -1.2955,\n         -0.7998,  -0.7509,  -1.3573,  -1.6845,  -1.9268,  -2.9515],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.7070, -13.6708, -60.9608, -47.9320, -75.2156,  -9.5885,  -8.6920,\n         -6.8742,  -5.6366,  -5.9192,  -4.5887,  -4.6155,  -5.0372,  -5.1457,\n         -5.7343,  -5.4993,  -5.9887,  -6.1759,  -6.5280,  -8.4194],\n       grad_fn=<SelectBackward>)\n \ntensor([ -0.5309,  -2.1399,  -9.5462,  -7.4786, -11.4960,  -1.5916,  -1.0125,\n         -1.9490,  -1.0369,  -0.8130,  -0.0629,  -0.1599,  -0.7167,  -0.9408,\n         -0.2259,  -0.4985,  -0.8709,  -1.3232,  -1.7627,  -2.4554],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.6725,  -5.9268, -26.9742, -21.1441, -32.9918,  -4.4780,  -3.8353,\n         -3.8182,  -3.0033,  -2.5136,  -1.8394,  -1.9287,  -2.4270,  -2.3340,\n         -1.9731,  -2.6842,  -2.4172,  -2.4746,  -3.5371,  -4.3392],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.3816,  -3.5268, -15.3262, -12.0947, -18.4743,  -2.3928,  -2.0155,\n         -2.6746,  -1.8880,  -1.4175,  -0.6803,  -0.3846,  -1.0304,  -1.3587,\n         -1.0158,  -1.3563,  -1.4345,  -1.7395,  -2.5679,  -3.3358],\n       grad_fn=<SelectBackward>)\n \ntensor([ -4.0489,  -9.0311, -40.9089, -32.2288, -50.5681,  -6.4348,  -5.6273,\n         -4.9898,  -3.9523,  -3.8380,  -2.9809,  -3.0842,  -3.3852,  -3.4448,\n         -3.6089,  -3.5380,  -3.9457,  -4.3609,  -4.6425,  -6.3657],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.1787,  -5.1643, -23.0612, -18.0594, -28.2273,  -3.8743,  -3.4159,\n         -3.4049,  -2.4695,  -2.0763,  -1.3050,  -1.3469,  -1.8468,  -2.0163,\n         -2.0276,  -2.1689,  -2.0758,  -2.5176,  -3.1857,  -3.8521],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.9873, -15.0769, -67.2926, -52.8953, -83.4996, -10.4453,  -9.7266,\n         -7.3435,  -6.0172,  -6.2209,  -5.0608,  -5.2593,  -5.7106,  -5.6551,\n         -6.0580,  -5.9237,  -6.7276,  -7.0168,  -7.0128,  -9.7170],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.5385,  -4.0445, -18.1284, -14.2704, -22.0743,  -2.9644,  -2.3324,\n         -2.9759,  -1.9583,  -1.7515,  -0.7880,  -0.7189,  -1.4687,  -1.6009,\n         -1.4817,  -1.6710,  -1.6866,  -2.0710,  -2.7765,  -3.5903],\n       grad_fn=<SelectBackward>)\n \ntensor([ -5.8043, -12.3762, -56.0104, -44.0708, -69.0812,  -8.9220,  -7.9208,\n         -6.4548,  -5.4326,  -5.4834,  -4.2767,  -4.2501,  -4.6537,  -4.6185,\n         -4.9643,  -5.0678,  -5.3255,  -5.5599,  -6.1848,  -7.9745],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.0400, -12.5883, -56.7460, -44.6032, -70.0582,  -8.9423,  -8.2267,\n         -6.6062,  -5.5138,  -5.2199,  -4.6399,  -4.3972,  -4.6292,  -4.8000,\n         -5.1288,  -4.9312,  -5.4030,  -5.6416,  -6.3051,  -8.2585],\n       grad_fn=<SelectBackward>)\n \ntensor([ -5.3526, -11.4799, -51.8725, -40.8947, -64.0988,  -8.3671,  -7.4436,\n         -6.1149,  -5.0715,  -4.9678,  -3.9774,  -3.9248,  -4.3512,  -4.2888,\n         -4.5912,  -4.6701,  -4.8875,  -5.2571,  -5.8786,  -7.5479],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.5993,  -6.1366, -26.5386, -20.9496, -32.8999,  -4.3217,  -3.8764,\n         -3.8689,  -2.7515,  -2.1903,  -1.6877,  -1.8260,  -1.9776,  -2.3123,\n         -2.2692,  -2.1234,  -2.5379,  -2.8854,  -3.4451,  -4.8186],\n       grad_fn=<SelectBackward>)\n \ntensor([ -7.3666, -15.3204, -69.5698, -54.4389, -85.9548, -10.8942, -10.1459,\n         -7.7297,  -6.5157,  -6.3316,  -5.4188,  -5.5710,  -5.9861,  -6.0039,\n         -6.4656,  -6.3559,  -6.7997,  -7.0936,  -7.3953,  -9.7692],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.4474, -13.5485, -61.0031, -47.6582, -75.2516,  -9.5970,  -9.0328,\n         -6.8021,  -5.7200,  -5.6814,  -4.7391,  -4.7878,  -5.3192,  -5.3414,\n         -5.6205,  -5.7103,  -5.9221,  -6.1109,  -6.4857,  -8.3731],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.5469,  -5.8958, -26.3755, -20.4903, -32.0797,  -4.1054,  -3.6721,\n         -3.7957,  -2.5348,  -2.1294,  -1.3251,  -1.5056,  -2.2724,  -2.4415,\n         -2.2462,  -2.3620,  -2.6610,  -2.9853,  -3.3355,  -4.6204],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.2313,  -5.2152, -22.3073, -17.7541, -27.3291,  -3.5456,  -3.1471,\n         -3.3278,  -2.4434,  -2.0392,  -1.4309,  -1.1484,  -1.6186,  -2.0045,\n         -1.8375,  -1.8220,  -2.3284,  -2.3657,  -2.9058,  -4.1948],\n       grad_fn=<SelectBackward>)\n \ntensor([ -5.3273, -11.2768, -50.9799, -40.0601, -62.7271,  -8.0886,  -7.3035,\n         -5.9149,  -4.9514,  -4.9718,  -3.9698,  -3.9090,  -4.3381,  -4.2346,\n         -4.3527,  -4.6142,  -4.9544,  -4.9153,  -5.5185,  -7.1751],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.4669, -13.5677, -61.3290, -48.1425, -75.5077,  -9.6407,  -8.7367,\n         -6.7118,  -5.5631,  -5.8722,  -4.6624,  -4.6447,  -5.3107,  -5.2149,\n         -5.5412,  -5.5748,  -6.0901,  -6.2249,  -6.3522,  -8.3382],\n       grad_fn=<SelectBackward>)\n \ntensor([ -4.8557, -10.1771, -46.3452, -36.5597, -57.0304,  -7.4720,  -6.4968,\n         -5.4577,  -4.6594,  -4.7176,  -3.4764,  -3.3612,  -3.7250,  -3.7614,\n         -4.1178,  -4.1939,  -4.4390,  -4.6229,  -5.2426,  -6.6135],\n       grad_fn=<SelectBackward>)\n \ntensor([ -5.7485, -12.6250, -55.7357, -44.0000, -69.0382,  -8.8207,  -7.9254,\n         -6.3940,  -5.2936,  -5.4346,  -4.3048,  -4.2730,  -4.4622,  -4.5421,\n         -5.1275,  -4.8094,  -5.5808,  -5.7616,  -5.9984,  -8.0023],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.2306, -12.8982, -58.9476, -46.4165, -72.6952,  -9.5516,  -8.4410,\n         -6.6064,  -5.5363,  -5.8354,  -4.4616,  -4.6331,  -4.9999,  -4.8845,\n         -5.3490,  -5.4534,  -5.5510,  -5.9369,  -6.4633,  -8.3104],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.0177, -12.3606, -56.0049, -44.0838, -69.0480,  -9.0465,  -8.1642,\n         -6.4565,  -5.3866,  -5.4219,  -4.4408,  -4.3035,  -4.6352,  -4.7885,\n         -5.1410,  -5.0837,  -5.2776,  -5.5213,  -6.2459,  -8.0042],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.1475,  -4.6986, -21.3905, -16.8000, -25.7942,  -3.6649,  -3.0100,\n         -3.2011,  -2.2885,  -1.9884,  -1.0715,  -1.2067,  -1.6693,  -1.8779,\n         -1.8190,  -1.9923,  -2.0356,  -2.3648,  -2.8300,  -3.7522],\n       grad_fn=<SelectBackward>)\n \ntensor([ -0.8273,  -2.2173,  -9.9937,  -7.9565, -11.8849,  -1.7734,  -1.0117,\n         -1.9715,  -1.3162,  -1.0342,  -0.2128,  -0.1891,  -0.6954,  -0.9594,\n         -0.4422,  -0.6566,  -0.7903,  -1.1380,  -1.8418,  -2.3253],\n       grad_fn=<SelectBackward>)\n \ntensor([ -4.6231,  -9.9946, -44.2642, -34.8978, -54.5407,  -6.9545,  -6.3516,\n         -5.3795,  -4.4151,  -4.2765,  -3.4202,  -2.9601,  -3.5126,  -3.6639,\n         -3.8108,  -3.9121,  -4.3864,  -4.3814,  -5.2130,  -6.7156],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.9369,  -6.6393, -29.3263, -23.3189, -36.2103,  -4.7937,  -4.1155,\n         -3.7358,  -3.1774,  -2.9658,  -2.3663,  -2.0124,  -2.4083,  -2.3658,\n         -2.1531,  -2.5793,  -2.8333,  -2.6656,  -3.6056,  -4.4811],\n       grad_fn=<SelectBackward>)\n \ntensor([-0.5351, -1.3658, -6.3025, -4.8556, -6.9891, -1.1283, -0.3937, -1.5651,\n        -0.6367, -0.4388,  0.2728,  0.2126,  0.0419, -0.5027, -0.3113, -0.1903,\n        -0.3057, -1.2193, -1.4977, -1.9037], grad_fn=<SelectBackward>)\n \ntensor([ -7.1209, -15.0251, -68.4567, -53.6468, -84.6084, -10.8266,  -9.9788,\n         -7.5240,  -6.2081,  -6.4880,  -5.1914,  -5.5168,  -5.9183,  -5.8013,\n         -6.2078,  -6.2524,  -6.6793,  -6.8899,  -6.9995,  -9.5435],\n       grad_fn=<SelectBackward>)\n \ntensor([ -4.4558,  -9.5549, -41.9164, -33.0707, -51.7992,  -6.6417,  -5.9680,\n         -5.0072,  -4.0240,  -3.9000,  -3.2559,  -2.9700,  -3.2436,  -3.4170,\n         -3.7696,  -3.5284,  -4.0676,  -4.2648,  -4.6684,  -6.2704],\n       grad_fn=<SelectBackward>)\n \ntensor([ -3.6999,  -7.7728, -34.9535, -27.5044, -42.8597,  -5.5900,  -4.8647,\n         -4.3459,  -3.6181,  -3.1461,  -2.6472,  -2.3868,  -2.8975,  -2.9451,\n         -2.9001,  -3.0472,  -3.4380,  -3.4782,  -4.1924,  -5.3351],\n       grad_fn=<SelectBackward>)\n \ntensor([ -7.2910, -14.9142, -68.2637, -53.3503, -84.0035, -10.6415,  -9.8539,\n         -7.5684,  -6.0989,  -6.2963,  -5.1194,  -5.2919,  -5.9496,  -5.9188,\n         -6.2279,  -6.3333,  -6.7155,  -6.9847,  -7.1952,  -9.5560],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.1787,  -3.2778, -14.3929, -11.5107, -17.5538,  -2.3347,  -1.8652,\n         -2.4772,  -1.7520,  -1.5632,  -0.6798,  -0.4001,  -1.0341,  -1.1433,\n         -0.8460,  -1.2382,  -1.3457,  -1.5171,  -2.3941,  -2.9160],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.4305, -13.0334, -60.4930, -47.3681, -74.2360,  -9.6685,  -8.6851,\n         -6.7617,  -5.5770,  -5.8459,  -4.5120,  -4.7673,  -5.3016,  -5.0920,\n         -5.3993,  -5.8384,  -5.6449,  -6.0356,  -6.5129,  -8.2035],\n       grad_fn=<SelectBackward>)\n \ntensor([ -7.4979, -15.2901, -69.2622, -54.2280, -85.4687, -10.9056, -10.1886,\n         -7.6786,  -6.5135,  -6.3572,  -5.5160,  -5.5868,  -5.9553,  -5.9669,\n         -6.2447,  -6.2646,  -6.5498,  -6.8082,  -7.4380,  -9.5170],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.0223,  -2.6695, -12.2407,  -9.9174, -14.6106,  -2.1739,  -1.3799,\n         -2.2416,  -1.4981,  -1.3148,  -0.6286,  -0.3705,  -0.7290,  -0.8474,\n         -0.7202,  -0.8908,  -0.8901,  -1.4177,  -2.0555,  -2.5101],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.1115,  -3.1390, -13.3821, -10.6835, -16.1613,  -2.0469,  -1.6084,\n         -2.4919,  -1.7312,  -1.1890,  -0.6714,  -0.3733,  -0.8928,  -1.1291,\n         -0.6838,  -1.1151,  -1.3322,  -1.5363,  -2.2555,  -3.0069],\n       grad_fn=<SelectBackward>)\n \ntensor([ -7.2707, -15.1810, -68.4354, -53.5746, -84.5848, -10.4659,  -9.9211,\n         -7.7152,  -6.1819,  -6.1295,  -5.2590,  -5.5267,  -5.8790,  -6.1709,\n         -6.4139,  -5.9500,  -6.8572,  -7.1668,  -7.2120,  -9.9422],\n       grad_fn=<SelectBackward>)\n \ntensor([ -4.8886, -10.8752, -49.6731, -38.9728, -61.0616,  -7.9101,  -6.9420,\n         -5.8278,  -4.6089,  -4.8941,  -3.5530,  -3.8275,  -4.2124,  -4.0206,\n         -4.3837,  -4.5405,  -4.7206,  -5.1577,  -5.4737,  -7.2341],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.3839,  -5.4641, -24.7817, -19.5876, -30.3095,  -4.1123,  -3.4479,\n         -3.6643,  -2.8182,  -2.4182,  -1.5064,  -1.3372,  -1.8499,  -1.9880,\n         -1.9752,  -2.2365,  -2.2652,  -2.6009,  -3.4948,  -4.2419],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.4481,  -5.7913, -26.4449, -20.7810, -32.3465,  -4.1682,  -3.7280,\n         -3.5914,  -2.7928,  -2.5543,  -1.7185,  -1.6430,  -2.4433,  -2.4191,\n         -1.8887,  -2.5754,  -2.6150,  -2.6319,  -3.3502,  -4.3344],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.3867,  -3.5060, -15.4305, -12.2931, -18.9836,  -2.4751,  -2.0485,\n         -2.5269,  -1.8254,  -1.3514,  -0.9167,  -0.5906,  -1.1936,  -1.3201,\n         -0.9275,  -1.3630,  -1.4608,  -1.5326,  -2.2549,  -3.2726],\n       grad_fn=<SelectBackward>)\n \ntensor([-4.0392e-01, -6.4743e-01, -4.0275e+00, -3.1917e+00, -3.8869e+00,\n        -6.8163e-01,  2.5993e-01, -1.3445e+00, -5.9505e-01, -9.0180e-02,\n         4.2828e-01,  5.9251e-01,  2.0298e-01, -3.3586e-01,  2.0841e-01,\n         2.4399e-03,  9.1516e-02, -8.4245e-01, -1.4105e+00, -1.5205e+00],\n       grad_fn=<SelectBackward>)\n \ntensor([ -3.4335,  -7.5604, -33.5988, -26.6219, -41.0975,  -5.3109,  -4.6428,\n         -4.4100,  -3.3362,  -3.3908,  -2.4208,  -1.9527,  -2.5590,  -2.7216,\n         -2.8569,  -3.0364,  -3.3051,  -3.4592,  -4.2509,  -5.3466],\n       grad_fn=<SelectBackward>)\n \ntensor([ -0.9475,  -3.0020, -13.4571, -10.6510, -16.3603,  -2.3031,  -1.8274,\n         -2.4015,  -1.6216,  -1.3303,  -0.5055,  -0.4951,  -1.0132,  -1.1875,\n         -0.8892,  -1.0893,  -1.3554,  -1.6004,  -2.1892,  -2.8218],\n       grad_fn=<SelectBackward>)\n \ntensor([ -3.0587,  -6.9908, -30.8230, -24.4308, -38.0506,  -5.1121,  -4.3524,\n         -3.9915,  -3.2600,  -3.0278,  -2.4535,  -2.2537,  -2.3149,  -2.3311,\n         -2.5555,  -2.6088,  -2.8740,  -3.0485,  -3.7719,  -4.7423],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.2112, -13.2679, -59.0238, -46.3196, -73.0373,  -9.2480,  -8.4755,\n         -6.7289,  -5.4628,  -5.4142,  -4.4016,  -4.5925,  -4.9766,  -5.0870,\n         -5.5734,  -5.2042,  -5.7612,  -6.1516,  -6.3371,  -8.5100],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.3802,  -3.8728, -17.6603, -13.8030, -21.8718,  -2.6116,  -2.5551,\n         -3.0878,  -2.1481,  -1.4042,  -0.8693,  -0.9113,  -1.5049,  -1.9456,\n         -1.2882,  -1.4253,  -1.8567,  -2.0355,  -2.7603,  -4.2852],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.5232, -13.9114, -62.6754, -49.0871, -77.2448,  -9.8528,  -9.0474,\n         -6.9452,  -5.7353,  -5.9003,  -4.5950,  -4.9118,  -5.2722,  -5.2643,\n         -5.6541,  -5.6073,  -6.1824,  -6.5060,  -6.5829,  -8.5438],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.8953,  -4.4885, -20.7870, -16.4853, -25.3878,  -3.6160,  -2.9212,\n         -3.2450,  -2.4855,  -2.0162,  -1.3253,  -1.1987,  -1.6488,  -1.7036,\n         -1.6101,  -1.9764,  -1.7951,  -2.1555,  -3.0552,  -3.7843],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.4388, -13.5426, -61.7685, -48.5874, -76.1633,  -9.7384,  -8.7547,\n         -6.6688,  -5.6991,  -6.1083,  -4.7410,  -4.7784,  -5.4494,  -5.1717,\n         -5.3835,  -5.7657,  -6.0224,  -6.1619,  -6.4786,  -8.3314],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.5833,  -4.1278, -18.4343, -14.5069, -22.4677,  -3.0171,  -2.5577,\n         -2.9118,  -2.0407,  -1.6912,  -0.7231,  -0.9418,  -1.4499,  -1.7555,\n         -1.3589,  -1.5219,  -1.9214,  -2.1756,  -2.6264,  -3.6315],\n       grad_fn=<SelectBackward>)\n \ntensor([-0.5465, -1.3829, -6.0022, -4.6604, -6.5414, -0.9245, -0.3299, -1.5873,\n        -0.6762, -0.2110,  0.1253,  0.3846, -0.1142, -0.5275, -0.0326, -0.1550,\n        -0.2212, -1.0691, -1.5621, -2.0378], grad_fn=<SelectBackward>)\n \ntensor([ -4.4777,  -9.4202, -42.3930, -33.5423, -52.1680,  -6.8346,  -5.9286,\n         -5.0305,  -4.2016,  -4.2660,  -3.2834,  -2.9356,  -3.5133,  -3.3884,\n         -3.5571,  -3.9365,  -3.9642,  -4.0374,  -4.8542,  -6.1686],\n       grad_fn=<SelectBackward>)\n \ntensor([ -3.7774,  -8.4853, -37.2115, -29.2702, -46.0610,  -5.8106,  -5.1865,\n         -4.8054,  -3.5604,  -3.2301,  -2.4505,  -2.5381,  -2.8980,  -3.2017,\n         -3.3403,  -2.9139,  -3.8252,  -4.0182,  -4.3478,  -6.2492],\n       grad_fn=<SelectBackward>)\n \ntensor([ -4.2800,  -9.4006, -41.8264, -33.0883, -51.6937,  -6.7062,  -5.8653,\n         -5.0013,  -4.0980,  -4.0248,  -3.2834,  -3.1037,  -3.2373,  -3.2403,\n         -3.5746,  -3.5882,  -3.9269,  -4.2288,  -4.8062,  -6.2966],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.2862,  -3.4889, -15.1455, -12.0223, -18.3810,  -2.5616,  -2.0077,\n         -2.5955,  -1.7183,  -1.3870,  -0.8031,  -0.6819,  -1.0184,  -1.2119,\n         -0.9899,  -1.1266,  -1.3098,  -1.7313,  -2.3265,  -3.0921],\n       grad_fn=<SelectBackward>)\n \ntensor([ -6.6409, -13.6719, -61.2529, -48.2105, -75.5938,  -9.7536,  -8.8367,\n         -6.8073,  -5.6783,  -5.9132,  -4.7491,  -4.6725,  -5.0326,  -5.1399,\n         -5.9177,  -5.5777,  -5.9792,  -6.2946,  -6.5035,  -8.3420],\n       grad_fn=<SelectBackward>)\n \ntensor([ -7.7980, -15.8623, -71.8517, -56.4564, -88.5279, -11.3122, -10.4311,\n         -8.0644,  -6.7778,  -6.7268,  -5.8841,  -5.7151,  -5.9109,  -5.9724,\n         -6.6919,  -6.3790,  -6.8376,  -7.1484,  -7.7117,  -9.9172],\n       grad_fn=<SelectBackward>)\n \ntensor([ -3.8230,  -8.4476, -38.3210, -30.0194, -47.3407,  -5.9779,  -5.4969,\n         -4.8166,  -3.8690,  -3.4243,  -2.7251,  -2.9505,  -3.2638,  -3.5183,\n         -3.3358,  -3.2965,  -3.8641,  -4.0431,  -4.4497,  -6.1667],\n       grad_fn=<SelectBackward>)\n \ntensor([ -1.7754,  -4.5662, -20.4822, -16.2093, -25.0052,  -3.4336,  -2.7679,\n         -3.2152,  -2.3650,  -2.0984,  -1.0202,  -0.9985,  -1.4843,  -1.6828,\n         -1.5454,  -1.8038,  -1.8402,  -2.1788,  -3.0886,  -3.7076],\n       grad_fn=<SelectBackward>)\n \ntensor([ -3.6253,  -7.5821, -35.4552, -27.8787, -43.4154,  -5.9622,  -5.0904,\n         -4.5919,  -3.7038,  -3.5057,  -2.6623,  -2.4364,  -2.8861,  -2.7533,\n         -2.9083,  -3.5539,  -2.9389,  -3.3425,  -4.6214,  -5.2850],\n       grad_fn=<SelectBackward>)\n \ntensor([ -4.7930, -10.2235, -45.7634, -36.0075, -56.3089,  -7.2305,  -6.3745,\n         -5.4666,  -4.2960,  -4.4939,  -3.2468,  -3.2631,  -3.6335,  -3.7645,\n         -4.1312,  -4.0273,  -4.4612,  -4.7618,  -5.0781,  -6.6855],\n       grad_fn=<SelectBackward>)\n \ntensor([ -2.4966,  -5.7691, -25.3684, -19.9319, -31.1048,  -3.9915,  -3.7217,\n         -3.6483,  -2.6819,  -2.2795,  -1.7917,  -1.5154,  -2.0178,  -2.2738,\n         -2.1513,  -2.1070,  -2.5152,  -2.6016,  -3.4335,  -4.3293],\n       grad_fn=<SelectBackward>)\n \ntensor([ -7.1410, -14.5538, -66.7643, -52.4924, -82.1138, -10.6706,  -9.6368,\n         -7.4006,  -6.0785,  -6.4901,  -5.0973,  -5.3091,  -5.6338,  -5.5208,\n         -6.2007,  -6.2582,  -6.2257,  -6.8426,  -7.1221,  -9.2220],\n       grad_fn=<SelectBackward>)\n \ntensor([ -5.3778, -11.4992, -51.6413, -40.6940, -63.6977,  -8.2995,  -7.3874,\n         -6.0585,  -5.0085,  -5.0504,  -3.8504,  -3.8166,  -4.1470,  -4.2390,\n         -4.6556,  -4.6333,  -4.8376,  -5.2047,  -5.8738,  -7.3942],\n       grad_fn=<SelectBackward>)\n \ntensor([ -7.2003, -15.0737, -68.6644, -53.7263, -84.7726, -10.7460,  -9.9720,\n         -7.5997,  -6.3387,  -6.4640,  -5.2597,  -5.4158,  -6.0954,  -6.0349,\n         -6.3204,  -6.4153,  -6.6084,  -6.8758,  -7.2624,  -9.6155],\n       grad_fn=<SelectBackward>)\n \ntensor([ -5.8026, -12.5612, -55.8405, -43.7345, -68.5858,  -8.6150,  -7.9202,\n         -6.6848,  -5.1801,  -5.0593,  -4.2727,  -4.0928,  -4.6356,  -4.8391,\n         -5.1524,  -4.7557,  -5.7676,  -5.7790,  -6.0784,  -8.2172],\n       grad_fn=<SelectBackward>)\n \n"
    }
   ],
   "source": [
    "for t in test_results:\n",
    "    print(t)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(35)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x_test_tensor)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "848      8\n529     12\n348     15\n805     17\n45       6\n377     10\n570      8\n226     15\n441     13\n457     10\n477     11\n149     10\n190     13\n907      7\n401     13\n251     10\n581     10\n419     10\n632     10\n594     14\n734     15\n542     14\n899     13\n780     10\n514     14\n787     15\n361     12\n880     11\n799     16\n20      15\n        ..\n692     10\n344     10\n367      0\n216      4\n42      18\n469     11\n186     11\n431     14\n827      7\n247      8\n876     11\n287     12\n863     13\n255      8\n75      10\n593     16\n522     11\n588     13\n209      7\n858     12\n1008    10\n537     12\n284     11\n494     13\n640     13\n356     13\n936     11\n724     14\n962      0\n779     10\nName: G3, Length: 261, dtype: int64"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}