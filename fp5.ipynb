{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38264bita821192d7e144cac81722f01973f0984",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Layer(torch.nn.Module):\n",
    "    def __init__(self, inputDim, outputDim, activation):\n",
    "        \n",
    "        super(Layer, self).__init__()\n",
    "        self.inputDim = inputDim\n",
    "        self.outputDim = outputDim\n",
    "        self.linear = torch.nn.Linear(inputDim, outputDim)\n",
    "        activation_functions = [torch.nn.Identity, torch.nn.ReLU, torch.nn.Tanh, torch.nn.Sigmoid]\n",
    "        activation_inputs = [\"Identity\", \"ReLU\", \"Tanh\", \"Sigmoid\"]\n",
    "        self.activation = activation_functions[activation_inputs.index(activation)]()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inp = self.linear(x)\n",
    "        output = self.activation(inp)\n",
    "        \n",
    "        return output\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        self.layers = layers\n",
    "        lst = []\n",
    "        for layer in layers:\n",
    "            lst += list(layer.parameters())\n",
    "        self.params = torch.nn.ParameterList(lst)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        temp_input = x\n",
    "        for layer in self.layers:\n",
    "            temp_input = layer.forward(temp_input)\n",
    "        return temp_input\n",
    "    def predict(self, data):\n",
    "        data_tensor = torch.FloatTensor(data)\n",
    "        return [torch.argmax(self.forward(item)).item() for item in data_tensor]\n",
    "    def score(self, x_test, y_test):\n",
    "        x_test_tensor = torch.FloatTensor(pd.DataFrame(x_test).values)\n",
    "        test_results = self.forward(x_test_tensor)\n",
    "        results = [torch.argmax(res).item() for res in test_results]\n",
    "        \n",
    "        total = 0\n",
    "        for i in range(len(results)):\n",
    "            if results[i] == y_test[i]:\n",
    "                total += 1\n",
    "\n",
    "        return total/len(y_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_construct(layer_dims, activation):\n",
    "    \n",
    "    return Neural_Network([Layer(layer_dims[i-1], layer_dims[i], activation) for i in range(1, len(layer_dims))])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def nn_trainer(model, X_train, y_train):\n",
    "    # Trains a model\n",
    "    # Input: model: the neural network to be trained (Neural_Network)\n",
    "    #        X_train: training for features\n",
    "    #        y_train: training data for target\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model.params, lr=learning_rate)\n",
    "\n",
    "    for t in range(5000):\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        x_train_tensor = torch.FloatTensor(X_train)\n",
    "        \n",
    "        y_pred = model.forward(x_train_tensor)\n",
    "        \n",
    "        # Compute and print loss.\n",
    "        loss = criterion(y_pred, torch.tensor(pd.DataFrame(y_train).values, dtype = torch.long).reshape(-1))\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model.parameters(), model.forward(x_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads data\n",
    "features = pd.read_csv(\"data/features.csv\")\n",
    "target = pd.read_csv(\"data/target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bins target values\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=4, encode = \"onehot-dense\", strategy = \"quantile\")\n",
    "\n",
    "target_discretized = discretizer.fit_transform(target[\"G3\"].values.reshape(-1, 1))\n",
    "target_new = []\n",
    "for row in target_discretized:\n",
    "    target_new.append(list(row).index(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data into training, testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target_new, random_state = 1001)\n",
    "X_train = X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_construct([96, 48, 4], \"Sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, activations = nn_trainer(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.44061302681992337"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "nn_test_score = model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5951468710089399"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "nn_train_score = model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}